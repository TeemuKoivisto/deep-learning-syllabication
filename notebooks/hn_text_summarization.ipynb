{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import bs4 as BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_hn_article(url):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    raw_byte_stream = response.read()\n",
    "    parsed_html = BeautifulSoup.BeautifulSoup(raw_byte_stream, 'html.parser')\n",
    "    comments = [tag.text for tag in parsed_html.find_all(class_='commtext')]\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "def has_alpha(s):\n",
    "    return re.search('[a-zA-Z]', s) is not None\n",
    "\n",
    "def has_punctuation(s):\n",
    "    return re.search(f\"[{string.punctuation}]\", s)\n",
    "\n",
    "def has_number(s):\n",
    "    return re.search(\"[0-9]\", s)\n",
    "\n",
    "def process_word(w, stemmer, swords):\n",
    "    wl = w.lower()\n",
    "    if wl in swords:\n",
    "        return None\n",
    "    if not has_alpha(wl) or len(wl) is 1:\n",
    "        return None\n",
    "    if (has_punctuation(wl) or has_number(wl)) and len(wl) is 2:\n",
    "        return None\n",
    "    w = stemmer.stem(wl)\n",
    "    if w in swords:\n",
    "        return None\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_freq_dict(comments) -> dict:\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(' '.join(comments))\n",
    "    stemmer = PorterStemmer()\n",
    "    freq = dict()\n",
    "    for word in words:\n",
    "        w = process_word(word, stemmer, stop_words)\n",
    "        if w is None:\n",
    "            continue\n",
    "        elif w in freq:\n",
    "            freq[w] += 1\n",
    "        else:\n",
    "            freq[w] = 1\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_scores(sentences, freq_table) -> dict:   \n",
    "    weights = dict()\n",
    "    for sent in sentences:\n",
    "        word_count = 0\n",
    "        uid = hash(sent)\n",
    "        for word_weight in freq_table:\n",
    "            if word_weight in sent.lower():\n",
    "                word_count += 1\n",
    "                if uid in weights:\n",
    "                    weights[uid] += freq_table[word_weight]\n",
    "                else:\n",
    "                    weights[uid] = freq_table[word_weight]\n",
    "        if uid in weights:\n",
    "            weights[uid] = weights[uid] / word_count\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sentence_score(weights) -> int:\n",
    "    return sum([weights[w] for w in weights]) / len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(sentences, weights, threshold, sent_count=20):\n",
    "    sntces = {}\n",
    "    scores = {}\n",
    "    for sent in sentences:\n",
    "        uid = hash(sent)\n",
    "        if uid in weights and weights[uid] >= threshold:\n",
    "            sntces[uid] = sent\n",
    "            scores[uid] = weights[uid]\n",
    "    sorted_scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "    keys = list(sorted_scores.keys())\n",
    "    summary = []\n",
    "    for i in range(min(sent_count, len(keys))):\n",
    "        uid = keys[i]\n",
    "        sent = sntces[uid]\n",
    "        summary.append(sent)\n",
    "    return ' '.join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = parse_hn_article('https://news.ycombinator.com/item?id=22552632')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_hn_thread(comments):\n",
    "    frequency_table = create_freq_dict(comments)\n",
    "    comment_sentences = [sent_tokenize(c) for c in comments]\n",
    "    sents = [sent for sentences in comment_sentences for sent in sentences]\n",
    "    scores = sentence_scores(sents, frequency_table)\n",
    "    threshold = average_sentence_score(scores)\n",
    "    summary = generate_summary(sents, scores, 2 * threshold)\n",
    "    return summary\n",
    "\n",
    "summ = summarize_hn_thread(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You can't. I'm guessing that isn't you. You can't sell what you don't have. No, it wasn't secrecy that killed them. Nobody wanted it. None. I have both a Magic Leap One and a HoloLens 1. Their problem hasn't been timing. i don't know what your company is. That you don't see it hyped on TV doesn't mean it is not there. I don't want a voice telling me what to do. The goal isn't to own a product. Don't write off VR just yet. It uses magnetic tracking and it just doesn't do well. Oh man, I didn't know he worked there... don't you need to be able to see what's actually going on and not be in VR land? I've got news for Magic Leap. General Magic, GetMagic.com, and now this (not that we didn't see it coming)... AR has no use case. > AR has no use case.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'They couldn\\'t do it. I just don\\'t know very many. The US hasn\\'t even started testing people. Don\\'t defend this. I did just that. Much like in the US. Less than 50 people have the virus. The Chinese are just people, like the rest of us. That didn\\'t work out too well. If you don\\'t mind, Where did you find this information? Haven\\'t seen much over at ORF.at. I don\\'t even look at it anymore. If people need stuff, theyâ€™ll go out. It isn\\'t an \"online forum\". It isn\\'t hard to parse at all. Isn\\'t this happening in many places? Japan didn\\'t really test much. The fact that people outside of China don\\'t know what WeChat is make sense. Regulation doesn\\'t have to be binary. It likely will.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = parse_hn_article('https://news.ycombinator.com/item?id=22547283')\n",
    "\n",
    "summarize_hn_thread(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
